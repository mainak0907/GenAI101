{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CSV Converted to SQL and then done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kn10B7Z4fR1",
        "outputId": "a25ca31d-62e6-418e-aa44-1313a3bb017e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain langchain-openai langchain-community langchain-experimental pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdsR8LzS4pg5",
        "outputId": "3ba7c66f-c86e-44e8-8943-d67a53fd6db0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-02 16:39:58--  https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\n",
            "Resolving web.stanford.edu (web.stanford.edu)... 171.67.215.200, 2607:f6d0:0:925a::ab43:d7c8\n",
            "Connecting to web.stanford.edu (web.stanford.edu)|171.67.215.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44225 (43K) [text/csv]\n",
            "Saving to: ‘titanic.csv’\n",
            "\n",
            "titanic.csv         100%[===================>]  43.19K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-01-02 16:39:59 (1.14 MB/s) - ‘titanic.csv’ saved [44225/44225]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv -O titanic.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N58e-6QJ4vqx",
        "outputId": "d059f00e-e811-44db-dacd-a969d8110af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(887, 8)\n",
            "['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Siblings/Spouses Aboard', 'Parents/Children Aboard', 'Fare']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "print(df.shape)\n",
        "print(df.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vYxdcUh44gi",
        "outputId": "52bc9d4f-43e9-4f90-d837-8fa7b1c80a11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "887"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.utilities import SQLDatabase\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\"sqlite:///titanic.db\")\n",
        "df.to_sql(\"titanic\", engine, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmdF3myl48GC",
        "outputId": "57fc20ef-5f83-438c-e2a8-a179e89bbaac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sqlite\n",
            "['titanic']\n",
            "[(1, 2, 'Master. Alden Gates Caldwell', 'male', 0.83, 0, 2, 29.0), (0, 3, 'Master. Eino Viljami Panula', 'male', 1.0, 4, 1, 39.6875), (1, 3, 'Miss. Eleanor Ileen Johnson', 'female', 1.0, 1, 1, 11.1333), (1, 2, 'Master. Richard F Becker', 'male', 1.0, 2, 1, 39.0), (1, 1, 'Master. Hudson Trevor Allison', 'male', 0.92, 1, 2, 151.55), (1, 3, 'Miss. Maria Nakid', 'female', 1.0, 0, 2, 15.7417), (0, 3, 'Master. Sidney Leonard Goodwin', 'male', 1.0, 5, 2, 46.9), (1, 3, 'Miss. Helene Barbara Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 3, 'Miss. Eugenie Baclini', 'female', 0.75, 2, 1, 19.2583), (1, 2, 'Master. Viljo Hamalainen', 'male', 0.67, 1, 1, 14.5), (1, 3, 'Master. Bertram Vere Dean', 'male', 1.0, 1, 2, 20.575), (1, 3, 'Master. Assad Alexander Thomas', 'male', 0.42, 0, 1, 8.5167), (1, 2, 'Master. Andre Mallet', 'male', 1.0, 0, 2, 37.0042), (1, 2, 'Master. George Sibley Richards', 'male', 0.83, 1, 1, 18.75)]\n"
          ]
        }
      ],
      "source": [
        "db = SQLDatabase(engine=engine)\n",
        "print(db.dialect)\n",
        "print(db.get_usable_table_names())\n",
        "print(db.run(\"SELECT * FROM titanic WHERE Age < 2;\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ty9LkV_4-F0",
        "outputId": "9448e710-37aa-434b-a745-04f8f0ce00db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-fireworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb6kiHBu5GLk",
        "outputId": "c3cb540e-af58-41e6-a3f8-3719663fa553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Fireworks AI: ··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"FIREWORKS_API_KEY\"):\n",
        "  os.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass(\"Enter API key for Fireworks AI: \")\n",
        "\n",
        "from langchain_fireworks import ChatFireworks\n",
        "\n",
        "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJOXR5jI5T-n"
      },
      "outputs": [],
      "source": [
        "from langchain_community.agent_toolkits import create_sql_agent\n",
        "\n",
        "agent_executor = create_sql_agent(llm, db=db, agent_type=\"openai-tools\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lul8nE_c5b43"
      },
      "outputs": [],
      "source": [
        "from langchain_community.agent_toolkits import create_sql_agent\n",
        "\n",
        "agent_executor = create_sql_agent(llm, db=db, agent_type=\"openai-tools\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ptvHq-35g_n",
        "outputId": "f2a8a5cb-545e-48eb-d36c-5c96eca50331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `sql_db_list_tables` with `{'tool_input': ''}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mtitanic\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `sql_db_schema` with `{'table_names': 'titanic'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m\n",
            "CREATE TABLE titanic (\n",
            "\t\"Survived\" BIGINT, \n",
            "\t\"Pclass\" BIGINT, \n",
            "\t\"Name\" TEXT, \n",
            "\t\"Sex\" TEXT, \n",
            "\t\"Age\" FLOAT, \n",
            "\t\"Siblings/Spouses Aboard\" BIGINT, \n",
            "\t\"Parents/Children Aboard\" BIGINT, \n",
            "\t\"Fare\" FLOAT\n",
            ")\n",
            "\n",
            "/*\n",
            "3 rows from titanic table:\n",
            "Survived\tPclass\tName\tSex\tAge\tSiblings/Spouses Aboard\tParents/Children Aboard\tFare\n",
            "0\t3\tMr. Owen Harris Braund\tmale\t22.0\t1\t0\t7.25\n",
            "1\t1\tMrs. John Bradley (Florence Briggs Thayer) Cumings\tfemale\t38.0\t1\t0\t71.2833\n",
            "1\t3\tMiss. Laina Heikkinen\tfemale\t26.0\t0\t0\t7.925\n",
            "*/\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `sql_db_query_checker` with `{'query': 'SELECT AVG Age FROM titanic WHERE Survived = 1'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mSELECT AVG(Age) FROM titanic WHERE Survived = 1\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `sql_db_query` with `{'query': 'SELECT AVG(Age) FROM titanic WHERE Survived = 1'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[(28.408391812865496,)]\u001b[0m\u001b[32;1m\u001b[1;3mThe average age of survivors is 28.41.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': \"what's the average age of survivors\",\n",
              " 'output': 'The average age of survivors is 28.41.'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.invoke({\"input\": \"what's the average age of survivors\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGykYK9y5-tD",
        "outputId": "eb9ad35a-2364-4b22-bf25-534ea5f81842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```python\n",
            "# Compute the correlation between 'Age' and 'Fare'\n",
            "correlation = df['Age'].corr(df['Fare'])\n",
            "\n",
            "# Print the correlation\n",
            "print(correlation)\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "ai_msg = llm.invoke(\n",
        "    \"I have a pandas DataFrame 'df' with columns 'Age' and 'Fare'. Write code to compute the correlation between the two columns. Return Markdown for a Python code snippet and nothing else.\"\n",
        ")\n",
        "print(ai_msg.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIe5afpJ6I5l",
        "outputId": "d6be8a9c-fc11-45d9-84a5-67de47a823af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32.30542018038331"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_experimental.tools import PythonAstREPLTool\n",
        "\n",
        "df = pd.read_csv(\"titanic.csv\")\n",
        "tool = PythonAstREPLTool(locals={\"df\": df})\n",
        "tool.invoke(\"df['Fare'].mean()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSG2rdXn6Xrn",
        "outputId": "36aa3682-d83c-47bf-e407-1523ca5af61d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_sJsVVzdmkyiHLZshjURMYvwJ', 'type': 'function', 'function': {'name': 'python_repl_ast', 'arguments': '{\"query\": \"df[\"}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 231, 'total_tokens': 252, 'completion_tokens': 21}, 'model_name': 'accounts/fireworks/models/llama-v3p1-70b-instruct', 'system_fingerprint': '', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-45be91f7-3239-47c9-9b41-dcca43a55b43-0', tool_calls=[{'name': 'python_repl_ast', 'args': {'query': 'df['}, 'id': 'call_sJsVVzdmkyiHLZshjURMYvwJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 231, 'output_tokens': 21, 'total_tokens': 252})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_with_tools = llm.bind_tools([tool], tool_choice=tool.name)\n",
        "response = llm_with_tools.invoke(\n",
        "    \"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\"\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mGDhrM46YL3",
        "outputId": "a2c22060-f5ce-44d8-80e8-6e88d1c091af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'python_repl_ast',\n",
              "  'args': {'query': 'df['},\n",
              "  'id': 'call_sJsVVzdmkyiHLZshjURMYvwJ',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.tool_calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTHQFkGr6bCR",
        "outputId": "4e998234-bf68-4086-a865-952e97bf3fa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'df['}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers.openai_tools import JsonOutputKeyToolsParser\n",
        "\n",
        "parser = JsonOutputKeyToolsParser(key_name=tool.name, first_tool_only=True)\n",
        "(llm_with_tools | parser).invoke(\n",
        "    \"I have a dataframe 'df' and want to know the correlation between the 'Age' and 'Fare' columns\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rfq2C2h6r3S"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "system = f\"\"\"You have access to a pandas dataframe `df`. \\\n",
        "Here is the output of `df.head().to_markdown()`:\n",
        "\n",
        "\\`\\`\\`\n",
        "{df.head().to_markdown()}\n",
        "\\`\\`\\`\n",
        "\n",
        "Given a user question, write the Python code to answer it. \\\n",
        "Don't assume you have access to any libraries other than built-in Python ones and pandas.\n",
        "Respond directly to the question once you have enough information to answer it.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            system,\n",
        "        ),\n",
        "        (\"human\", \"{question}\"),\n",
        "        # This MessagesPlaceholder allows us to optionally append an arbitrary number of messages\n",
        "        # at the end of the prompt using the 'chat_history' arg.\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def _get_chat_history(x: dict) -> list:\n",
        "    \"\"\"Parse the chain output up to this point into a list of chat history messages to insert in the prompt.\"\"\"\n",
        "    ai_msg = x[\"ai_msg\"]\n",
        "    tool_call_id = x[\"ai_msg\"].additional_kwargs[\"tool_calls\"][0][\"id\"]\n",
        "    tool_msg = ToolMessage(tool_call_id=tool_call_id, content=str(x[\"tool_output\"]))\n",
        "    return [ai_msg, tool_msg]\n",
        "\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(ai_msg=prompt | llm_with_tools)\n",
        "    .assign(tool_output=itemgetter(\"ai_msg\") | parser | tool)\n",
        "    .assign(chat_history=_get_chat_history)\n",
        "    .assign(response=prompt | llm | StrOutputParser())\n",
        "    .pick([\"tool_output\", \"response\"])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OqL3RPFz6wFS",
        "outputId": "fd65ef5c-4dc7-447d-e614-c8c9747aaddb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"SyntaxError: '[' was never closed (<unknown>, line 1)\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = prompt | llm_with_tools | parser | tool\n",
        "chain.invoke({\"question\": \"What's the correlation between age and fare\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

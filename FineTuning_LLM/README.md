# Finetuning LLM 

https://chatgpt.com/share/67701118-74c4-800f-abba-d43fec5922f1 - This ChatGPT chat consists of jsonl , parametre efficient transfer learning , rank of a matrix and ways to calculate it , rank of a matrix in matrix decomposition and Fine Tuning in LLM , Full Precision and Half Precision in Quantization , bf16 format , nf4 quantization , tokenizer in LLM , SFTrainer , Adapter Model in LLM Finetuning  

## Quantization 
![image](https://github.com/user-attachments/assets/14d9152c-624e-454b-9844-ba36082ba405)
![image](https://github.com/user-attachments/assets/99144845-de6d-4d78-b186-16c0fae30384)
![image](https://github.com/user-attachments/assets/c7ba6fe3-48d3-49d7-8163-021ed7f521be)
![image](https://github.com/user-attachments/assets/8a398975-dc7d-470c-a4dd-00a218308565)
![image](https://github.com/user-attachments/assets/aa69b064-6e0a-4814-a9a1-dbda2bbf51d9)
![image](https://github.com/user-attachments/assets/9f239788-288f-4d4d-97b1-8cc55d8cb2ba)
![image](https://github.com/user-attachments/assets/90904129-70ee-4602-96fc-1ca488de50bf)

## LoRA and QLoRA
![image](https://github.com/user-attachments/assets/595264ab-d435-40c3-a296-a1967bb0ff7d)
![image](https://github.com/user-attachments/assets/e80b85db-c06f-464c-bdc9-4f9456c93d31)
#### LoRA
![image](https://github.com/user-attachments/assets/db3db500-20c8-4b08-a239-bbfb8b106540)
![image](https://github.com/user-attachments/assets/86d111e4-5369-4cc2-9a23-ba7cd6655b59)

# QLoRA is basically quantized LoRA , parameters are quantized and then LoRA is applied 



